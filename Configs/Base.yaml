data:
  train: "./Dataset/phoenix2014-release/phoenix-2014-signerindependent-SI5/features/fullFrame-210x260px/train"
  dev: "./Dataset/phoenix2014-release/phoenix-2014-signerindependent-SI5/features/fullFrame-210x260px/dev"
  test: "./Dataset/phoenix2014-release/phoenix-2014-signerindependent-SI5/features/fullFrame-210x260px/test"

train:
  random_seed: 27   # Random seed for initialisation
  optimizer: "adam"   # Chosen optimiser (adam, ..)
  learning_rate: 0.001   # Initial model learning rate
  learning_rate_min: 0.0002 # Learning rate minimum, when training will stop
  weight_decay: 0.0   # Weight Decay
  clip_grad_norm: 5.0   # Gradient clipping value
  batch_size: 8    # Batch Size for training
  scheduling: "plateau"   # Scheduling at training time (plateau, ...)
  patience: 7  # How many epochs of no improvement causes a LR reduction
  decrease_factor: 0.7  # LR reduction factor, after the # of patience epochs
  early_stopping_metric: "dtw" # Which metric determines scheduling (DTW, loss, BT...)
  epochs: 200  # How many epochs to run for
  validation_freq: 10  # After how many steps to run a validation on the model
  logging_freq: 250  # After how many steps to log training progress
  eval_metric: "dtw"  # Evaluation metric during training (dtw','bt')
  model_dir: "./Trained_Models/Base" # Where the model shall be stored
  overwrite: False # Flag to overwrite a previous saved model in the model_dir
  continue: True  # Flag to continue from a previous saved model in the model_dir
  shuffle: True  # Flag to shuffle the data during training
  use_cuda: True  # Flag to use GPU cuda capabilities
  max_output_length: 300 # Max Output Length
  keep_last_ckpts: 1 # How many previous best/latest checkpoints to keep
  loss: "MSE"  # Loss function (MSE, L1)
