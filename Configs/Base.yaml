data:
  train_folder: "./Dataset/phoenix2014-release/phoenix-2014-signerindependent-SI5/features/fullFrame-210x260px/train"
  dev_folder: "./Dataset/phoenix2014-release/phoenix-2014-signerindependent-SI5/features/fullFrame-210x260px/dev"
  test_folder: "./Dataset/phoenix2014-release/phoenix-2014-signerindependent-SI5/features/fullFrame-210x260px/test"

  annotation_csv_train: "./Dataset/phoenix2014-release/phoenix-2014-signerindependent-SI5/annotations/manual/train.corpus.csv"

train:
  optimizer: "adam"
  epochs: 200  # 500
  learning_rate: 0.0001
  alpha: 0.1
  beta: 0.001
  batch_size: 8    # 64
  n_STGP_block: 4
  stage1loss: ["L2", "diversity"]
  stage2loss: ["cross_entropy", "latent_alignment"]

  learning_rate_min: 0.0002 # Learning rate minimum, when training will stop
  weight_decay: 0.0   # Weight Decay
  clip_grad_norm: 5.0   # Gradient clipping value
  scheduling: "plateau"   # Scheduling at training time (plateau, ...)
  patience: 7  # How many epochs of no improvement causes a LR reduction
  decrease_factor: 0.7  # LR reduction factor, after the # of patience epochs
  early_stopping_metric: "dtw" # Which metric determines scheduling (DTW, loss, BT...)
  validation_freq: 10  # After how many steps to run a validation on the model
  logging_freq: 250  # After how many steps to log training progress
  eval_metric: "dtw"  # Evaluation metric during training (dtw','bt')
  model_dir: "./Trained_Models/Base" # Where the model shall be stored
  overwrite: False # Flag to overwrite a previous saved model in the model_dir
  continue: True  # Flag to continue from a previous saved model in the model_dir
  shuffle: True  # Flag to shuffle the data during training
  use_cuda: True  # Flag to use GPU cuda capabilities
  max_output_length: 300 # Max Output Length
  keep_last_ckpts: 1 # How many previous best/latest checkpoints to keep
  loss: "MSE"  # Loss function (MSE, L1)
